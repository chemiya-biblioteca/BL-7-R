install.packages("caret")
library(caret)
Data <- read.csv("GenderClassification.csv",
stringsAsFactors = TRUE)
Data
set.seed(10)
Data$Favorite.Color <- as.numeric(Data$Favorite.Color)
Data$Favorite.Music.Genre <- as.numeric(Data$Favorite.Music.Genre)
Data$Favorite.Beverage <- as.numeric(Data$Favorite.Beverage)
Data$Favorite.Soft.Drink <- as.numeric(Data$Favorite.Soft.Drink)
Data
TrainingSize <- createDataPartition(Data$Gender,
p = 0.8,
list = FALSE)
TrainingSize
TrainingData <- Data[TrainingSize,]
TrainingData
TestingData <- Data[-TrainingSize,]
TestingData
model <- train(Gender ~ ., data = TrainingData,
method = "svmPoly",
na.action = na.omit,
preProcess = c("scale", "center"),
trControl = trainControl(method = "none"),
tuneGrid = data.frame(degree = 1,
scale = 1,
C = 1)
)
print(model)
ggplot(Data, aes(Favorite.Color)) +
geom_bar(fill = "#0073C2FF")
install.packages("randomForest")
library(randomForest)
?randomForest
model <- randomForest(formula = Gender ~ .,
data = Data)
print(model)
install.packages("randomForest")
install.packages("randomForest")
library(randomForest)
Data <- read.csv("GenderClassification.csv",
stringsAsFactors = TRUE)
?randomForest
model <- randomForest(formula = Gender ~ .,
data = Data)
print(model)
install.packages("caTools")
install.packages("randomForest")
install.packages("randomForest")
library(caTools)
library(randomForest)
split <- sample.split(iris, SplitRatio = 0.7)
split
train <- subset(iris, split == "TRUE")
test <- subset(iris, split == "FALSE")
set.seed(120)
classifier_RF = randomForest(x = train[-5],
y = train$Species,
ntree = 500)
classifier_RF
y_pred = predict(classifier_RF, newdata = test[-5])
confusion_mtx = table(test[, 5], y_pred)
confusion_mtx
library(nnet)
Data <- read.csv("GenderClassification.csv",
stringsAsFactors = TRUE)
model <- nnet(formula = Gender ~ .,
data = Data,
size = 30)
print(model)
library(e1071)
Data <- read.csv("GenderClassification.csv",
stringsAsFactors = TRUE)
model <- svm(formula = Gender ~ .,
data = Data)
print(model)
library(rpart)
Data <- read.csv("GenderClassification.csv",
stringsAsFactors = TRUE)
partition <- rpart(formula = Gender~.,
data = Data)
plot(partition)
library(dplyr)
Data <- read.csv("GenderClassification.csv",
stringsAsFactors = TRUE)
Data %>%
filter(Gender == "M")
dataset = read.csv('Salary_Data.csv')
install.packages('caTools')
install.packages("caTools")
library(caTools)
dataset
split = sample.split(dataset$Salary,
SplitRatio = 0.7)
split
trainingset = subset(dataset, split == TRUE)
trainingset
testset = subset(dataset, split == FALSE)
testset
lm.r = lm(formula = Salary ~ YearsExperience,
data = trainingset)
coef(lm.r)
ypred = predict(lm.r, newdata = testset)
data<-data.frame(testset,ypred)
data
install.packages("ggplot2")
install.packages("ggplot2")
library(ggplot2)
ggplot() + geom_point(aes(x = trainingset$YearsExperience,
y = trainingset$Salary),
colour = 'red') +
geom_line(aes(x = trainingset$YearsExperience,
y = predict(lm.r, newdata = trainingset)),
colour = 'blue') +
ggtitle('Salary vs Experience (Training set)') +
xlab('Years of experience') +
ylab('Salary')
ggplot() + geom_point(aes(x = testset$YearsExperience,
y = testset$Salary),
colour = 'red') +
geom_line(aes(x = trainingset$YearsExperience,
y = predict(lm.r, newdata = trainingset)),
colour = 'blue') +
ggtitle('Salary vs Experience (Test set)') +
xlab('Years of experience') +
ylab('Salary')
install.packages("ClusterR")
install.packages("cluster")
library(ClusterR)
library(cluster)
data(iris)
str(iris)
iris_1 <- iris[, -5]
iris_1
set.seed(240)
kmeans.re <- kmeans(iris_1, centers = 3,
nstart = 20)
kmeans.re
kmeans.re$cluster
cm <- table(iris$Species, kmeans.re$cluster)
cm
plot(iris_1[c("Sepal.Length", "Sepal.Width")])
plot(iris_1[c("Sepal.Length", "Sepal.Width")],
col = kmeans.re$cluster)
plot(iris_1[c("Sepal.Length", "Sepal.Width")],
col = kmeans.re$cluster,
main = "K-means with 3 clusters")
kmeans.re$centers
kmeans.re$centers[, c("Sepal.Length",
"Sepal.Width")]
points(kmeans.re$centers[, c("Sepal.Length",
"Sepal.Width")],
col = 1:3, pch = 8, cex = 3)
y_kmeans <- kmeans.re$cluster
clusplot(iris_1[, c("Sepal.Length", "Sepal.Width")],
y_kmeans,
lines = 0,
shade = TRUE,
color = TRUE,
labels = 2,
plotchar = FALSE,
span = TRUE,
main = paste("Cluster iris"),
xlab = 'Sepal.Length',
ylab = 'Sepal.Width')
#----------------------------------
install.packages("factoextra")
library(factoextra)
df <- mtcars
df <- na.omit(df)
df <- scale(df)
km <- kmeans(df, centers = 4, nstart = 25)
fviz_cluster(km, data = df)
km <- kmeans(df, centers = 5, nstart = 25)
fviz_cluster(km, data = df)
x <- c(153, 169, 140, 186, 128,
136, 178, 163, 152, 133)
y <- c(64, 81, 58, 91, 47, 57,
75, 72, 62, 49)
model <- lm(y~x)
print(model)
df <- data.frame(x = 182)
res <-  predict(model, df)
cat("\nPredicted value of a person
with height = 182")
print(res)
plot(x, y, main = "Height vs Weight
Regression model")
abline(lm(y~x))
airquality
input <- airquality[1:50,
c("Ozone", "Wind", "Temp")]
model <- lm(Ozone~Wind + Temp,
data = input)
cat("Regression model:\n")
print(model)
plot(model)
model <- glm(formula = vs ~ wt,
family = binomial,
data = mtcars)
x <- seq(min(mtcars$wt),
max(mtcars$wt),
0.01)
x
y <- predict(model, list(wt = x),
type = "response")
print(model)
plot(mtcars$wt, mtcars$vs, pch = 16,
xlab = "Weight", ylab = "VS")
lines(x, y)
install.packages("party")
library(party)
readingSkills
input.data <- readingSkills[c(1:105), ]
output.tree <- ctree(
nativeSpeaker ~ age + shoeSize + score,
data = input.data)
plot(output.tree)
library(datasets)
library(caTools)
library(party)
library(dplyr)
library(magrittr)
data("readingSkills")
head(readingSkills)
sample_data = sample.split(readingSkills, SplitRatio = 0.8)
train_data <- subset(readingSkills, sample_data == TRUE)
test_data <- subset(readingSkills, sample_data == FALSE)
model<- ctree(nativeSpeaker ~ ., train_data)
plot(model)
predict_model<-predict(model, test_data)
m_at <- table(test_data$nativeSpeaker, predict_model)
m_at
ac_Test <- sum(diag(m_at)) / sum(m_at)
print(paste('Accuracy for test is found to be', ac_Test))
install.packages("e1071")
install.packages("caTools")
install.packages("e1071")
install.packages("caret")
library(e1071)
library(caTools)
library(caret)
split <- sample.split(iris, SplitRatio = 0.7)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
train_cl
train_scale <- scale(train_cl[, 1:4])
train_scale
test_scale <- scale(test_cl[, 1:4])
set.seed(120)
classifier_cl <- naiveBayes(Species ~ ., data = train_cl)
classifier_cl
y_pred <- predict(classifier_cl, newdata = test_cl)
cm <- table(test_cl$Species, y_pred)
cm
confusionMatrix(cm)
install.packages("e1071")
install.packages("e1071")
install.packages("caTools")
install.packages("caTools")
install.packages("class")
install.packages("class")
library(e1071)
library(caTools)
library(class)
split <- sample.split(iris, SplitRatio = 0.7)
train_cl <- subset(iris, split == "TRUE")
test_cl <- subset(iris, split == "FALSE")
train_scale <- scale(train_cl[, 1:4])
test_scale <- scale(test_cl[, 1:4])
classifier_knn <- knn(train = train_scale,
test = test_scale,
cl = train_cl$Species,
k = 1)
classifier_knn
cm <- table(test_cl$Species, classifier_knn)
cm
misClassError <- mean(classifier_knn != test_cl$Species)
print(paste('Accuracy =', 1-misClassError))
classifier_knn <- knn(train = train_scale,
test = test_scale,
cl = train_cl$Species,
k = 5)
misClassError <- mean(classifier_knn != test_cl$Species)
print(paste('Accuracy =', 1-misClassError))
